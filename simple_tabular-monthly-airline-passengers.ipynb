{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\" Example script for predicting columns of tables, demonstrating simple use-case \"\"\"\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# Training time:\n",
    "train_data = TabularDataset('../../modified/monthly-airline-passengers-train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "train_data = train_data.head(500)  # subsample for faster demo\n",
    "print(train_data.head())\n",
    "label = 'class'  # specifies which column do we want to predict\n",
    "save_path = 'monthly_airline_passengers_ms/'  # where to save trained models\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         0  class\n",
      "0  1954-07    302\n",
      "1  1960-03    419\n",
      "2  1958-10    359\n",
      "3  1951-05    172\n",
      "4  1957-09    404\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=3600)\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"monthly_airline_passengers_ms/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    115\n",
      "Train Data Columns: 1\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (622, 104, 280.27826, 121.7046)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    510056.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['0']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['0']\n",
      "\t0.0s = Fit runtime\n",
      "\t1 features in original data used to generate 1 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.0 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 92, Val Rows: 23\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.94s of the 3599.94s of remaining time.\n",
      "\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.94s of the 3599.94s of remaining time.\n",
      "\tNo valid features to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.94s of the 3599.93s of remaining time.\n",
      "\t-279.205\t = Validation root_mean_squared_error score\n",
      "\t1.37s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3598.55s of the 3598.55s of remaining time.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1000]\ttrain_set's rmse: 47.6893\tvalid_set's rmse: 44.1087\n",
      "[2000]\ttrain_set's rmse: 47.3477\tvalid_set's rmse: 43.9162\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\t-43.8825\t = Validation root_mean_squared_error score\n",
      "\t2.93s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 3595.53s of the 3595.53s of remaining time.\n",
      "\t-52.4017\t = Validation root_mean_squared_error score\n",
      "\t0.84s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3594.56s of the 3594.56s of remaining time.\n",
      "\t-53.715\t = Validation root_mean_squared_error score\n",
      "\t0.18s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 3594.37s of the 3594.37s of remaining time.\n",
      "\t-52.1288\t = Validation root_mean_squared_error score\n",
      "\t0.94s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3593.29s of the 3593.28s of remaining time.\n",
      "\t-302.2315\t = Validation root_mean_squared_error score\n",
      "\t3.97s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3589.28s of the 3589.28s of remaining time.\n",
      "\t-51.3535\t = Validation root_mean_squared_error score\n",
      "\t0.35s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ... Training model for up to 3588.91s of the 3588.91s of remaining time.\n",
      "\t-75.1129\t = Validation root_mean_squared_error score\n",
      "\t3.65s\t = Training runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3585.15s of the 3585.15s of remaining time.\n",
      "\t-56.916\t = Validation root_mean_squared_error score\n",
      "\t1.48s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3583.2s of remaining time.\n",
      "\t-40.73\t = Validation root_mean_squared_error score\n",
      "\t0.42s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 17.24s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"monthly_airline_passengers_ms/\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model   score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2  -40.729976       0.247097  11.807750                0.000610           0.423252            2       True         10\n",
      "1             LightGBM  -43.882464       0.012847   2.925854                0.012847           2.925854            1       True          2\n",
      "2              XGBoost  -51.353526       0.002909   0.352035                0.002909           0.352035            1       True          7\n",
      "3        ExtraTreesMSE  -52.128770       0.119126   0.943112                0.119126           0.943112            1       True          5\n",
      "4      RandomForestMSE  -52.401664       0.116745   0.836100                0.116745           0.836100            1       True          3\n",
      "5             CatBoost  -53.714984       0.003253   0.182872                0.003253           0.182872            1       True          4\n",
      "6        LightGBMLarge  -56.915961       0.007995   1.481615                0.007995           1.481615            1       True          9\n",
      "7       NeuralNetMXNet  -75.112935       0.103167   3.648180                0.103167           3.648180            1       True          8\n",
      "8           LightGBMXT -279.205003       0.006810   1.367083                0.006810           1.367083            1       True          1\n",
      "9      NeuralNetFastAI -302.231542       0.013729   3.974364                0.013729           3.974364            1       True          6\n",
      "Number of models trained: 10\n",
      "Types of models trained:\n",
      "{'NNFastAiTabularModel', 'TabularNeuralNetModel', 'CatBoostModel', 'XTModel', 'RFModel', 'LGBModel', 'XGBoostModel', 'WeightedEnsembleModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('int', ['datetime_as_int']) : 1 | ['0']\n",
      "Plot summary of models saved to file: monthly_airline_passengers_ms/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Inference time:\n",
    "test_data = TabularDataset('../../modified/monthly-airline-passengers-test.csv')  # another Pandas DataFrame\n",
    "y_test = test_data[label]\n",
    "test_data = test_data.drop(labels=[label], axis=1)  # delete labels from test data since we wouldn't have them in practice\n",
    "print(test_data.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loaded data from: ../../modified/monthly-airline-passengers-test.csv | Columns = 2 / 2 | Rows = 29 -> 29\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         0\n",
      "0  1954-02\n",
      "1  1956-05\n",
      "2  1956-11\n",
      "3  1958-05\n",
      "4  1950-05\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "predictor = TabularPredictor.load(save_path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -39.39104401378377\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -39.39104401378377,\n",
      "    \"mean_squared_error\": -1551.65434849585,\n",
      "    \"mean_absolute_error\": -28.988385430697736,\n",
      "    \"r2\": 0.8782109828689734,\n",
      "    \"pearsonr\": 0.9383370926842507,\n",
      "    \"median_absolute_error\": -25.0328369140625\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print('Accuracy = {:.2f}'.format(perf[\"root_mean_squared_error\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = -39.39\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_data.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           0  class\n",
       "110  1950-12    140\n",
       "111  1953-07    264\n",
       "112  1949-07    148\n",
       "113  1959-05    420\n",
       "114  1958-01    340"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1950-12</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1953-07</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1949-07</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1959-05</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1958-01</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=3600)\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n",
    "import autogluon.core as ag\n",
    "\n",
    "nn_options = {  # specifies non-default hyperparameter values for neural network models\n",
    "    'num_epochs': 10,  # number of training epochs (controls training time of NN models)\n",
    "    'learning_rate': ag.space.Real(1e-4, 1e-2, default=5e-4, log=True),  # learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "    'activation': ag.space.Categorical('relu', 'softrelu', 'tanh'),  # activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "    'layers': ag.space.Categorical([100], [1000], [200, 100], [300, 200, 100]),  # each choice for categorical hyperparameter 'layers' corresponds to list of sizes for each NN layer to use\n",
    "    'dropout_prob': ag.space.Real(0.0, 0.5, default=0.1),  # dropout probability (real-valued hyperparameter)\n",
    "}\n",
    "\n",
    "gbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)\n",
    "    'num_leaves': ag.space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)\n",
    "}\n",
    "\n",
    "hyperparameters = {  # hyperparameters of each model type\n",
    "                   'GBM': gbm_options,\n",
    "                   'NN': nn_options,  # NOTE: comment this line out if you get errors on Mac OSX\n",
    "                  }  # When these keys are missing from hyperparameters dict, no models of that type are trained\n",
    "\n",
    "time_limit = 2*60  # train various models for ~2 min\n",
    "num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using Bayesian optimization routine with a local scheduler\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy,\n",
    "}\n",
    "\n",
    "path = 'monthly_airline_passengers_additional_hypers_ms/'  # where to save trained models\n",
    "\n",
    "predictor = TabularPredictor(label=label, path=path).fit(\n",
    "    train_data, time_limit=3600,\n",
    "    hyperparameters=hyperparameters, hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"monthly_airline_passengers_ms/\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"monthly_airline_passengers_ms/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    115\n",
      "Train Data Columns: 1\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (622, 104, 280.27826, 121.7046)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    507970.0 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['0']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['0']\n",
      "\t0.1s = Fit runtime\n",
      "\t1 features in original data used to generate 1 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.0 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 92, Val Rows: 23\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.92s of the 3599.92s of remaining time.\n",
      "\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.92s of the 3599.92s of remaining time.\n",
      "\tNo valid features to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.92s of the 3599.91s of remaining time.\n",
      "\t-279.205\t = Validation root_mean_squared_error score\n",
      "\t2.51s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3597.39s of the 3597.39s of remaining time.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1000]\ttrain_set's rmse: 47.6893\tvalid_set's rmse: 44.1087\n",
      "[2000]\ttrain_set's rmse: 47.3477\tvalid_set's rmse: 43.9162\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\t-43.8825\t = Validation root_mean_squared_error score\n",
      "\t6.3s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 3591.01s of the 3591.0s of remaining time.\n",
      "\t-52.4017\t = Validation root_mean_squared_error score\n",
      "\t0.86s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3590.01s of the 3590.01s of remaining time.\n",
      "\t-53.715\t = Validation root_mean_squared_error score\n",
      "\t0.1s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 3589.9s of the 3589.9s of remaining time.\n",
      "\t-52.1288\t = Validation root_mean_squared_error score\n",
      "\t0.85s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3588.91s of the 3588.91s of remaining time.\n",
      "\t-302.624\t = Validation root_mean_squared_error score\n",
      "\t0.84s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3588.04s of the 3588.04s of remaining time.\n",
      "\t-51.3535\t = Validation root_mean_squared_error score\n",
      "\t1.2s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ... Training model for up to 3586.81s of the 3586.81s of remaining time.\n",
      "\t-72.2862\t = Validation root_mean_squared_error score\n",
      "\t4.09s\t = Training runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3582.57s of the 3582.56s of remaining time.\n",
      "\t-56.916\t = Validation root_mean_squared_error score\n",
      "\t1.92s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3580.14s of remaining time.\n",
      "\t-41.1568\t = Validation root_mean_squared_error score\n",
      "\t0.43s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.31s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"monthly_airline_passengers_ms/\")\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"monthly_airline_passengers_additional_hypers_ms/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    115\n",
      "Train Data Columns: 1\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (622, 104, 280.27826, 121.7046)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    507920.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model   score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2  -41.156763       0.123720  7.591243                0.000678           0.429753            2       True         10\n",
      "1             LightGBM  -43.882464       0.006022  6.301794                0.006022           6.301794            1       True          2\n",
      "2              XGBoost  -51.353526       0.003671  1.203275                0.003671           1.203275            1       True          7\n",
      "3        ExtraTreesMSE  -52.128770       0.118657  0.850056                0.118657           0.850056            1       True          5\n",
      "4      RandomForestMSE  -52.401664       0.117020  0.859696                0.117020           0.859696            1       True          3\n",
      "5             CatBoost  -53.714984       0.002997  0.103866                0.002997           0.103866            1       True          4\n",
      "6        LightGBMLarge  -56.915961       0.004158  1.916424                0.004158           1.916424            1       True          9\n",
      "7       NeuralNetMXNet  -72.286214       0.153839  4.087160                0.153839           4.087160            1       True          8\n",
      "8           LightGBMXT -279.205003       0.002934  2.514126                0.002934           2.514126            1       True          1\n",
      "9      NeuralNetFastAI -302.624015       0.009710  0.843906                0.009710           0.843906            1       True          6\n",
      "Number of models trained: 10\n",
      "Types of models trained:\n",
      "{'NNFastAiTabularModel', 'TabularNeuralNetModel', 'CatBoostModel', 'XTModel', 'RFModel', 'LGBModel', 'XGBoostModel', 'WeightedEnsembleModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('int', ['datetime_as_int']) : 1 | ['0']\n",
      "Plot summary of models saved to file: monthly_airline_passengers_ms/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['0']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['0']\n",
      "\t0.0s = Fit runtime\n",
      "\t1 features in original data used to generate 1 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.0 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 92, Val Rows: 23\n",
      "Hyperparameter tuning model: LightGBM ...\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.31it/s]\n",
      "Fitted model: LightGBM/T0 ...\n",
      "\t-44.7611\t = Validation root_mean_squared_error score\n",
      "\t1.51s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitted model: LightGBM/T1 ...\n",
      "\t-57.0352\t = Validation root_mean_squared_error score\n",
      "\t0.23s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitted model: LightGBM/T2 ...\n",
      "\t-44.1268\t = Validation root_mean_squared_error score\n",
      "\t0.32s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitted model: LightGBM/T3 ...\n",
      "\t-48.0599\t = Validation root_mean_squared_error score\n",
      "\t1.1s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitted model: LightGBM/T4 ...\n",
      "\t-50.2524\t = Validation root_mean_squared_error score\n",
      "\t0.27s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetMXNet ...\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.70s/it]\n",
      "Fitted model: NeuralNetMXNet/T0 ...\n",
      "\t-123.652\t = Validation root_mean_squared_error score\n",
      "\t0.36s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T1 ...\n",
      "\t-94.8762\t = Validation root_mean_squared_error score\n",
      "\t0.31s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T2 ...\n",
      "\t-116.3295\t = Validation root_mean_squared_error score\n",
      "\t0.3s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T3 ...\n",
      "\t-74.1711\t = Validation root_mean_squared_error score\n",
      "\t0.32s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: NeuralNetMXNet/T4 ...\n",
      "\t-93.7516\t = Validation root_mean_squared_error score\n",
      "\t0.35s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3585.31s of remaining time.\n",
      "\t-43.306\t = Validation root_mean_squared_error score\n",
      "\t0.49s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.2s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"monthly_airline_passengers_additional_hypers_ms/\")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "predictor = TabularPredictor.load(path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -41.46283804397411\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -41.46283804397411,\n",
      "    \"mean_squared_error\": -1719.1669386608269,\n",
      "    \"mean_absolute_error\": -30.958954909752155,\n",
      "    \"r2\": 0.8650629555824572,\n",
      "    \"pearsonr\": 0.9354652075779957,\n",
      "    \"median_absolute_error\": -20.561065673828125\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluations on test data:\n",
    "{\n",
    "    \"root_mean_squared_error\": -39.39104401378377,\n",
    "    \"mean_squared_error\": -1551.65434849585,\n",
    "    \"mean_absolute_error\": -28.988385430697736,\n",
    "    \"r2\": 0.8782109828689734,\n",
    "    \"pearsonr\": 0.9383370926842507,\n",
    "    \"median_absolute_error\": -25.0328369140625\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('auto_glucon': venv)"
  },
  "interpreter": {
   "hash": "8cbcbecefa271004e6582b9d6d4515a215f41598e734b38ac4197b8836b69e1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}