{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"\"\" Example script for predicting columns of tables, demonstrating simple use-case \"\"\"\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# Training time:\n",
    "data = TabularDataset('./datasets/nlp-getting-started/train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "#train_data = train_data.head(500)  # subsample for faster demo\n",
    "print(data.head())\n",
    "label = 'target'  # specifies which column do we want to predict\n",
    "save_path = 'sentiment_analysis_all_instances/'  # where to save trained models\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "rows = len(data.index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_data = data.iloc[:int(0.8*rows),]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test_data = data.iloc[int(0.8*rows):,]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "test_data.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id keyword location  \\\n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=3600, presets='best_quality')\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"sentiment_analysis/\"\n",
      "Presets specified: ['best_quality']\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"sentiment_analysis/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    6090\n",
      "Train Data Columns: 4\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    430915.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.74 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 494\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['id']\n",
      "\t\t('object', [])       : 2 | ['keyword', 'location']\n",
      "\t\t('object', ['text']) : 1 | ['text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   2 | ['keyword', 'location']\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['text']\n",
      "\t\t('int', [])                         :   1 | ['id']\n",
      "\t\t('int', ['binned', 'text_special']) :  38 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 495 | ['__nlp__.11', '__nlp__.2015', '__nlp__.30', '__nlp__.40', '__nlp__.70', ...]\n",
      "\t3.0s = Fit runtime\n",
      "\t4 features in original data used to generate 537 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.42 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2397.34s of the 3596.88s of remaining time.\n",
      "\t0.6849\t = Validation accuracy score\n",
      "\t0.07s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2397.12s of the 3596.66s of remaining time.\n",
      "\t0.6754\t = Validation accuracy score\n",
      "\t0.06s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2396.9s of the 3596.44s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 286. Best iteration is:\n",
      "\t[166]\ttrain_set's binary_error: 0.126802\tvalid_set's binary_error: 0.164204\n",
      "\tRan out of time, early stopping on iteration 300. Best iteration is:\n",
      "\t[233]\ttrain_set's binary_error: 0.108192\tvalid_set's binary_error: 0.192118\n",
      "\t0.7985\t = Validation accuracy score\n",
      "\t2082.32s\t = Training runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 314.21s of the 1513.75s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\t[24]\ttrain_set's binary_error: 0.186645\tvalid_set's binary_error: 0.241379\n",
      "\tRan out of time, early stopping on iteration 27. Best iteration is:\n",
      "\t[23]\ttrain_set's binary_error: 0.182448\tvalid_set's binary_error: 0.238095\n",
      "\tRan out of time, early stopping on iteration 27. Best iteration is:\n",
      "\t[25]\ttrain_set's binary_error: 0.188104\tvalid_set's binary_error: 0.221675\n",
      "\tRan out of time, early stopping on iteration 28. Best iteration is:\n",
      "\t[27]\ttrain_set's binary_error: 0.183726\tvalid_set's binary_error: 0.233169\n",
      "\tRan out of time, early stopping on iteration 29. Best iteration is:\n",
      "\t[24]\ttrain_set's binary_error: 0.186827\tvalid_set's binary_error: 0.231527\n",
      "\tRan out of time, early stopping on iteration 31. Best iteration is:\n",
      "\t[23]\ttrain_set's binary_error: 0.192666\tvalid_set's binary_error: 0.223317\n",
      "\tRan out of time, early stopping on iteration 32. Best iteration is:\n",
      "\t[26]\ttrain_set's binary_error: 0.180989\tvalid_set's binary_error: 0.239737\n",
      "\tRan out of time, early stopping on iteration 35. Best iteration is:\n",
      "\t[20]\ttrain_set's binary_error: 0.190111\tvalid_set's binary_error: 0.226601\n",
      "\tRan out of time, early stopping on iteration 40. Best iteration is:\n",
      "\t[26]\ttrain_set's binary_error: 0.18409\tvalid_set's binary_error: 0.228243\n",
      "\tRan out of time, early stopping on iteration 49. Best iteration is:\n",
      "\t[44]\ttrain_set's binary_error: 0.165663\tvalid_set's binary_error: 0.206897\n",
      "\t0.7709\t = Validation accuracy score\n",
      "\t304.13s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 9.73s of the 1209.27s of remaining time.\n",
      "\t0.7573\t = Validation accuracy score\n",
      "\t1.44s\t = Training runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 7.04s of the 1206.58s of remaining time.\n",
      "\t0.764\t = Validation accuracy score\n",
      "\t1.44s\t = Training runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4.65s of the 1204.2s of remaining time.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 4.02s of the 1203.56s of remaining time.\n",
      "\t0.7695\t = Validation accuracy score\n",
      "\t1.42s\t = Training runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1.52s of the 1201.06s of remaining time.\n",
      "\t0.7765\t = Validation accuracy score\n",
      "\t1.39s\t = Training runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1198.6s of remaining time.\n",
      "\t0.7987\t = Validation accuracy score\n",
      "\t1.75s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L2 ... Training model for up to 1196.83s of the 1196.8s of remaining time.\n",
      "\t0.6887\t = Validation accuracy score\n",
      "\t0.08s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L2 ... Training model for up to 1196.59s of the 1196.56s of remaining time.\n",
      "\t0.6916\t = Validation accuracy score\n",
      "\t0.08s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1196.35s of the 1196.32s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 127. Best iteration is:\n",
      "\t[37]\ttrain_set's binary_error: 0.180259\tvalid_set's binary_error: 0.172414\n",
      "\tRan out of time, early stopping on iteration 130. Best iteration is:\n",
      "\t[21]\ttrain_set's binary_error: 0.187192\tvalid_set's binary_error: 0.190476\n",
      "\tRan out of time, early stopping on iteration 136. Best iteration is:\n",
      "\t[15]\ttrain_set's binary_error: 0.18701\tvalid_set's binary_error: 0.229885\n",
      "\tRan out of time, early stopping on iteration 138. Best iteration is:\n",
      "\t[15]\ttrain_set's binary_error: 0.195585\tvalid_set's binary_error: 0.174056\n",
      "\tRan out of time, early stopping on iteration 145. Best iteration is:\n",
      "\t[62]\ttrain_set's binary_error: 0.164569\tvalid_set's binary_error: 0.208539\n",
      "\tRan out of time, early stopping on iteration 148. Best iteration is:\n",
      "\t[24]\ttrain_set's binary_error: 0.186462\tvalid_set's binary_error: 0.187192\n",
      "\tRan out of time, early stopping on iteration 156. Best iteration is:\n",
      "\t[31]\ttrain_set's binary_error: 0.182084\tvalid_set's binary_error: 0.20197\n",
      "\t0.8007\t = Validation accuracy score\n",
      "\t1122.54s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 73.44s of the 73.41s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's binary_error: 0.420544\tvalid_set's binary_error: 0.422003\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 64.83s of the 64.8s of remaining time.\n",
      "\t0.7962\t = Validation accuracy score\n",
      "\t1.41s\t = Training runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 62.46s of the 62.43s of remaining time.\n",
      "\t0.7928\t = Validation accuracy score\n",
      "\t1.4s\t = Training runtime\n",
      "\t1.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 59.78s of the 59.75s of remaining time.\n",
      "\t0.8049\t = Validation accuracy score\n",
      "\t28.31s\t = Training runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 30.93s of the 30.9s of remaining time.\n",
      "\t0.7961\t = Validation accuracy score\n",
      "\t1.44s\t = Training runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 28.55s of the 28.52s of remaining time.\n",
      "\t0.7974\t = Validation accuracy score\n",
      "\t1.39s\t = Training runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 26.21s of the 26.18s of remaining time.\n",
      "\tRan out of time, stopping training early.\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 20.35s of the 20.32s of remaining time.\n",
      "\t0.8021\t = Validation accuracy score\n",
      "\t19.56s\t = Training runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L2 ... Training model for up to 0.37s of the 0.35s of remaining time.\n",
      "\tTime limit exceeded... Skipping NeuralNetMXNet_BAG_L2.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -2.36s of remaining time.\n",
      "\t0.8049\t = Validation accuracy score\n",
      "\t1.86s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3604.26s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"sentiment_analysis/\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           CatBoost_BAG_L2   0.804926       4.213077  2420.577271                0.467541          28.306417            2       True         15\n",
      "1       WeightedEnsemble_L3   0.804926       4.222094  2422.437293                0.009017           1.860021            3       True         19\n",
      "2            XGBoost_BAG_L2   0.802135       4.045090  2411.830435                0.299554          19.559580            2       True         18\n",
      "3         LightGBMXT_BAG_L2   0.800657       3.977075  3514.814481                0.231538        1122.543626            2       True         12\n",
      "4       WeightedEnsemble_L2   0.798686       2.921751  2392.569174                0.009828           1.752869            2       True          9\n",
      "5         LightGBMXT_BAG_L1   0.798522       0.224371  2082.322057                0.224371        2082.322057            1       True          3\n",
      "6     ExtraTreesEntr_BAG_L2   0.797373       4.506766  2393.660740                0.761230           1.389886            2       True         17\n",
      "7   RandomForestGini_BAG_L2   0.796223       4.563521  2393.684980                0.817985           1.414125            2       True         13\n",
      "8     ExtraTreesGini_BAG_L2   0.796059       4.506013  2393.715705                0.760477           1.444850            2       True         16\n",
      "9   RandomForestEntr_BAG_L2   0.792775       4.889863  2393.672347                1.144327           1.401493            2       True         14\n",
      "10    ExtraTreesEntr_BAG_L1   0.776519       0.716967     1.393045                0.716967           1.393045            1       True          8\n",
      "11          LightGBM_BAG_L1   0.770936       0.227012   304.129541                0.227012         304.129541            1       True          4\n",
      "12    ExtraTreesGini_BAG_L1   0.769458       0.728193     1.417853                0.728193           1.417853            1       True          7\n",
      "13  RandomForestEntr_BAG_L1   0.764039       0.638329     1.443030                0.638329           1.443030            1       True          6\n",
      "14  RandomForestGini_BAG_L1   0.757307       0.975952     1.437984                0.975952           1.437984            1       True          5\n",
      "15    KNeighborsDist_BAG_L2   0.691626       3.862386  2392.353351                0.116850           0.082497            2       True         11\n",
      "16    KNeighborsUnif_BAG_L2   0.688670       3.864314  2392.350340                0.118778           0.079485            2       True         10\n",
      "17    KNeighborsUnif_BAG_L1   0.684893       0.118066     0.065842                0.118066           0.065842            1       True          1\n",
      "18    KNeighborsDist_BAG_L1   0.675369       0.116647     0.061504                0.116647           0.061504            1       True          2\n",
      "Number of models trained: 19\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB'}\n",
      "Bagging used: True  (with 10 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :   2 | ['keyword', 'location']\n",
      "('category', ['text_as_category'])  :   1 | ['text']\n",
      "('int', [])                         :   1 | ['id']\n",
      "('int', ['binned', 'text_special']) :  38 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "('int', ['text_ngram'])             : 495 | ['__nlp__.11', '__nlp__.2015', '__nlp__.30', '__nlp__.40', '__nlp__.70', ...]\n",
      "Plot summary of models saved to file: sentiment_analysis/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Inference time:\n",
    "# test_data = TabularDataset('./datasets/nlp-getting-started/test.csv')  # another Pandas DataFrame\n",
    "y_test = test_data[label]\n",
    "test_data = test_data.drop(labels=[label], axis=1)  # delete labels from test data since we wouldn't have them in practice\n",
    "print(test_data.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        id  keyword                   location  \\\n",
      "6090  8697  sinking  North East Unsigned Radio   \n",
      "6091  8698  sinking   Every Where in the World   \n",
      "6092  8699  sinking                    Memphis   \n",
      "6093  8700  sinking                        NaN   \n",
      "6094  8702  sinking                        NaN   \n",
      "\n",
      "                                                                                                                                           text  \n",
      "6090                                      #nowplaying Sinking Fast - Now or Never on North East Unsigned Radio listen at http://t.co/QymAlttvZp  \n",
      "6091          that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time  \n",
      "6092                                                     Nigga car sinking but he snapping it up for fox 13. #priorities http://t.co/9StLKH59Fb  \n",
      "6093  @abandonedpics You should delete this one it's not an abbandoned nor sinking. ThatÛªs the darsena of the Castello scaligero di Sirmione.  \n",
      "6094          that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "predictor = TabularPredictor.load(save_path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: accuracy on test data: 0.7334208798424163\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.7334208798424163,\n",
      "    \"balanced_accuracy\": 0.7211406174734807,\n",
      "    \"mcc\": 0.47845757438928815,\n",
      "    \"f1\": 0.6547619047619048,\n",
      "    \"precision\": 0.8244111349036403,\n",
      "    \"recall\": 0.5430183356840621\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print('Accuracy = {:.2f}%'.format(perf[\"accuracy\"] * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 73.34%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test_data.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       0          1       2             3   4                   5  \\\n",
       "9764  30    Private  151868           9th   5  Married-civ-spouse   \n",
       "9765  32  State-gov  104509       HS-grad   9            Divorced   \n",
       "9766  22    Private  187592  Some-college  10       Never-married   \n",
       "9767  32    Private   49539  Some-college  10       Never-married   \n",
       "9768  25    Private  102476     Bachelors  13       Never-married   \n",
       "\n",
       "                    6              7      8       9     10  11  12  \\\n",
       "9764     Craft-repair        Husband  White    Male      0   0  40   \n",
       "9765    Other-service  Not-in-family  White  Female      0   0  25   \n",
       "9766  Exec-managerial  Not-in-family  White    Male      0   0  30   \n",
       "9767    Other-service  Not-in-family  White  Female   3674   0  40   \n",
       "9768  Farming-fishing      Own-child  White    Male  27828   0  50   \n",
       "\n",
       "                 13  \n",
       "9764  United-States  \n",
       "9765  United-States  \n",
       "9766  United-States  \n",
       "9767  United-States  \n",
       "9768  United-States  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9764</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>151868</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9765</th>\n",
       "      <td>32</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>104509</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9766</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>187592</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9767</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>49539</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>3674</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9768</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>102476</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>27828</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('auto_glucon': venv)"
  },
  "interpreter": {
   "hash": "8cbcbecefa271004e6582b9d6d4515a215f41598e734b38ac4197b8836b69e1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}