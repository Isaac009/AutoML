{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\" Example script for predicting columns of tables, demonstrating simple use-case \"\"\"\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# Training time:\n",
    "data = TabularDataset('./datasets/nlp-getting-started/train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "#train_data = train_data.head(500)  # subsample for faster demo\n",
    "print(data.head())\n",
    "label = 'target'  # specifies which column do we want to predict\n",
    "save_path = 'sentiment_analysis_all_instances/'  # where to save trained models\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "rows = len(data.index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_data = data.iloc[:int(0.8*rows),]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "test_data = data.iloc[int(0.8*rows):,]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "test_data.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id keyword location  \\\n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=7200, presets='best_quality')\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Beginning AutoGluon training ... Time limit = 7200s\n",
      "AutoGluon will save models to \"sentiment_analysis_all_instances/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    6090\n",
      "Train Data Columns: 4\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    423631.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.74 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 494\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['id']\n",
      "\t\t('object', [])       : 2 | ['keyword', 'location']\n",
      "\t\t('object', ['text']) : 1 | ['text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   2 | ['keyword', 'location']\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['text']\n",
      "\t\t('int', [])                         :   1 | ['id']\n",
      "\t\t('int', ['binned', 'text_special']) :  38 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 495 | ['__nlp__.11', '__nlp__.2015', '__nlp__.30', '__nlp__.40', '__nlp__.70', ...]\n",
      "\t3.0s = Fit runtime\n",
      "\t4 features in original data used to generate 537 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.42 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 4796.77s of the 7196.93s of remaining time.\n",
      "\t0.6849\t = Validation accuracy score\n",
      "\t0.06s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 4796.55s of the 7196.71s of remaining time.\n",
      "\t0.6754\t = Validation accuracy score\n",
      "\t0.07s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4796.33s of the 7196.49s of remaining time.\n",
      "\t0.7985\t = Validation accuracy score\n",
      "\t2184.19s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2611.75s of the 5011.91s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 300. Best iteration is:\n",
      "\t[222]\ttrain_set's binary_error: 0.0640394\tvalid_set's binary_error: 0.203612\n",
      "\tRan out of time, early stopping on iteration 303. Best iteration is:\n",
      "\t[294]\ttrain_set's binary_error: 0.0439701\tvalid_set's binary_error: 0.213465\n",
      "\t0.7977\t = Validation accuracy score\n",
      "\t2316.44s\t = Training runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 294.87s of the 2695.03s of remaining time.\n",
      "\t0.7573\t = Validation accuracy score\n",
      "\t1.46s\t = Training runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 292.2s of the 2692.36s of remaining time.\n",
      "\t0.764\t = Validation accuracy score\n",
      "\t1.42s\t = Training runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 289.64s of the 2689.8s of remaining time.\n",
      "\t0.7946\t = Validation accuracy score\n",
      "\t69.68s\t = Training runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 219.32s of the 2619.48s of remaining time.\n",
      "\t0.7695\t = Validation accuracy score\n",
      "\t1.44s\t = Training runtime\n",
      "\t1.45s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 216.24s of the 2616.4s of remaining time.\n",
      "\t0.7765\t = Validation accuracy score\n",
      "\t1.4s\t = Training runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 213.71s of the 2613.87s of remaining time.\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 7: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "No improvement since epoch 1: early stopping\n",
      "\t0.7842\t = Validation accuracy score\n",
      "\t99.79s\t = Training runtime\n",
      "\t2.76s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 110.23s of the 2510.39s of remaining time.\n",
      "\t0.7589\t = Validation accuracy score\n",
      "\t106.16s\t = Training runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ... Training model for up to 3.54s of the 2403.7s of remaining time.\n",
      "\tTime limit exceeded... Skipping NeuralNetMXNet_BAG_L1.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1.08s of the 2401.24s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's binary_error: 0.420544\tvalid_set's binary_error: 0.422003\n",
      "\tTime limit exceeded... Skipping LightGBMLarge_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 479.67s of the 2390.31s of remaining time.\n",
      "\t0.8061\t = Validation accuracy score\n",
      "\t2.45s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L2 ... Training model for up to 2387.82s of the 2387.79s of remaining time.\n",
      "\t0.6882\t = Validation accuracy score\n",
      "\t0.09s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L2 ... Training model for up to 2387.58s of the 2387.55s of remaining time.\n",
      "\t0.6969\t = Validation accuracy score\n",
      "\t0.09s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2387.33s of the 2387.3s of remaining time.\n",
      "\t0.8085\t = Validation accuracy score\n",
      "\t1410.68s\t = Training runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 976.29s of the 976.26s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 104. Best iteration is:\n",
      "\t[32]\ttrain_set's binary_error: 0.152344\tvalid_set's binary_error: 0.175698\n",
      "\tRan out of time, early stopping on iteration 106. Best iteration is:\n",
      "\t[39]\ttrain_set's binary_error: 0.147601\tvalid_set's binary_error: 0.190476\n",
      "\tRan out of time, early stopping on iteration 109. Best iteration is:\n",
      "\t[15]\ttrain_set's binary_error: 0.167123\tvalid_set's binary_error: 0.220033\n",
      "\tRan out of time, early stopping on iteration 113. Best iteration is:\n",
      "\t[19]\ttrain_set's binary_error: 0.163291\tvalid_set's binary_error: 0.165846\n",
      "\tRan out of time, early stopping on iteration 116. Best iteration is:\n",
      "\t[44]\ttrain_set's binary_error: 0.145594\tvalid_set's binary_error: 0.20197\n",
      "\tRan out of time, early stopping on iteration 122. Best iteration is:\n",
      "\t[76]\ttrain_set's binary_error: 0.122423\tvalid_set's binary_error: 0.188834\n",
      "\tRan out of time, early stopping on iteration 127. Best iteration is:\n",
      "\t[29]\ttrain_set's binary_error: 0.153074\tvalid_set's binary_error: 0.195402\n",
      "\tRan out of time, early stopping on iteration 138. Best iteration is:\n",
      "\t[8]\ttrain_set's binary_error: 0.18555\tvalid_set's binary_error: 0.205255\n",
      "\tRan out of time, early stopping on iteration 152. Best iteration is:\n",
      "\t[24]\ttrain_set's binary_error: 0.162744\tvalid_set's binary_error: 0.175698\n",
      "\tRan out of time, early stopping on iteration 184. Best iteration is:\n",
      "\t[90]\ttrain_set's binary_error: 0.115672\tvalid_set's binary_error: 0.205255\n",
      "\t0.8076\t = Validation accuracy score\n",
      "\t943.11s\t = Training runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 32.78s of the 32.75s of remaining time.\n",
      "\t0.7992\t = Validation accuracy score\n",
      "\t1.4s\t = Training runtime\n",
      "\t1.42s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 29.82s of the 29.79s of remaining time.\n",
      "\t0.8025\t = Validation accuracy score\n",
      "\t1.31s\t = Training runtime\n",
      "\t1.39s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 26.99s of the 26.96s of remaining time.\n",
      "\t0.8105\t = Validation accuracy score\n",
      "\t23.38s\t = Training runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 3.07s of the 3.04s of remaining time.\n",
      "\t0.8011\t = Validation accuracy score\n",
      "\t1.44s\t = Training runtime\n",
      "\t1.36s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 0.09s of the 0.06s of remaining time.\n",
      "\t0.7985\t = Validation accuracy score\n",
      "\t1.57s\t = Training runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -2.89s of remaining time.\n",
      "\t0.8105\t = Validation accuracy score\n",
      "\t1.97s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7204.9s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"sentiment_analysis_all_instances/\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           CatBoost_BAG_L2   0.810509       9.232457  4805.488914                0.463946          23.381783            2       True         19\n",
      "1       WeightedEnsemble_L3   0.810509       9.242150  4807.459924                0.009693           1.971010            3       True         22\n",
      "2         LightGBMXT_BAG_L2   0.808539       9.008408  6192.786363                0.239897        1410.679232            2       True         15\n",
      "3           LightGBM_BAG_L2   0.807553       9.051061  5725.218244                0.282549         943.111113            2       True         16\n",
      "4       WeightedEnsemble_L2   0.806076       7.815250  4783.163246                0.010120           2.451510            2       True         12\n",
      "5   RandomForestEntr_BAG_L2   0.802463      10.159657  4783.413818                1.391145           1.306687            2       True         18\n",
      "6     ExtraTreesGini_BAG_L2   0.801149      10.133376  4783.550940                1.364865           1.443810            2       True         20\n",
      "7   RandomForestGini_BAG_L2   0.799179      10.185267  4783.511371                1.416756           1.404240            2       True         17\n",
      "8         LightGBMXT_BAG_L1   0.798522       0.225220  2184.188051                0.225220        2184.188051            1       True          3\n",
      "9     ExtraTreesEntr_BAG_L2   0.798522       9.961872  4783.679962                1.193361           1.572831            2       True         21\n",
      "10          LightGBM_BAG_L1   0.797701       0.261539  2316.440656                0.261539        2316.440656            1       True          4\n",
      "11          CatBoost_BAG_L1   0.794581       0.522562    69.684652                0.522562          69.684652            1       True          7\n",
      "12   NeuralNetFastAI_BAG_L1   0.784236       2.756252    99.789374                2.756252          99.789374            1       True         10\n",
      "13    ExtraTreesEntr_BAG_L1   0.776519       0.963381     1.395395                0.963381           1.395395            1       True          9\n",
      "14    ExtraTreesGini_BAG_L1   0.769458       1.450506     1.435621                1.450506           1.435621            1       True          8\n",
      "15  RandomForestEntr_BAG_L1   0.764039       0.963169     1.423947                0.963169           1.423947            1       True          6\n",
      "16           XGBoost_BAG_L1   0.758949       0.344656   106.155616                0.344656         106.155616            1       True         11\n",
      "17  RandomForestGini_BAG_L1   0.757307       1.046713     1.459042                1.046713           1.459042            1       True          5\n",
      "18    KNeighborsDist_BAG_L2   0.696880       8.883803  4782.200673                0.115291           0.093542            2       True         14\n",
      "19    KNeighborsUnif_BAG_L2   0.688177       8.884139  4782.198022                0.115627           0.090891            2       True         13\n",
      "20    KNeighborsUnif_BAG_L1   0.684893       0.118317     0.064983                0.118317           0.064983            1       True          1\n",
      "21    KNeighborsDist_BAG_L1   0.675369       0.116196     0.069793                0.116196           0.069793            1       True          2\n",
      "Number of models trained: 22\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_RF', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_NNFastAiTabular', 'WeightedEnsembleModel', 'StackerEnsembleModel_CatBoost'}\n",
      "Bagging used: True  (with 10 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :   2 | ['keyword', 'location']\n",
      "('category', ['text_as_category'])  :   1 | ['text']\n",
      "('int', [])                         :   1 | ['id']\n",
      "('int', ['binned', 'text_special']) :  38 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "('int', ['text_ngram'])             : 495 | ['__nlp__.11', '__nlp__.2015', '__nlp__.30', '__nlp__.40', '__nlp__.70', ...]\n",
      "Plot summary of models saved to file: sentiment_analysis_all_instances/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Inference time:\n",
    "# test_data = TabularDataset('./datasets/nlp-getting-started/test.csv')  # another Pandas DataFrame\n",
    "y_test = test_data[label]\n",
    "test_data = test_data.drop(labels=[label], axis=1)  # delete labels from test data since we wouldn't have them in practice\n",
    "print(test_data.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        id  keyword                   location  \\\n",
      "6090  8697  sinking  North East Unsigned Radio   \n",
      "6091  8698  sinking   Every Where in the World   \n",
      "6092  8699  sinking                    Memphis   \n",
      "6093  8700  sinking                        NaN   \n",
      "6094  8702  sinking                        NaN   \n",
      "\n",
      "                                                                                                                                           text  \n",
      "6090                                      #nowplaying Sinking Fast - Now or Never on North East Unsigned Radio listen at http://t.co/QymAlttvZp  \n",
      "6091          that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time  \n",
      "6092                                                     Nigga car sinking but he snapping it up for fox 13. #priorities http://t.co/9StLKH59Fb  \n",
      "6093  @abandonedpics You should delete this one it's not an abbandoned nor sinking. ThatÛªs the darsena of the Castello scaligero di Sirmione.  \n",
      "6094          that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "predictor = TabularPredictor.load(save_path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: accuracy on test data: 0.7426132632961261\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.7426132632961261,\n",
      "    \"balanced_accuracy\": 0.7301039980870729,\n",
      "    \"mcc\": 0.49966250311989013,\n",
      "    \"f1\": 0.664957264957265,\n",
      "    \"precision\": 0.8438177874186551,\n",
      "    \"recall\": 0.5486600846262342\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print('Accuracy = {:.2f}%'.format(perf[\"accuracy\"] * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 73.34%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test_data.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       0          1       2             3   4                   5  \\\n",
       "9764  30    Private  151868           9th   5  Married-civ-spouse   \n",
       "9765  32  State-gov  104509       HS-grad   9            Divorced   \n",
       "9766  22    Private  187592  Some-college  10       Never-married   \n",
       "9767  32    Private   49539  Some-college  10       Never-married   \n",
       "9768  25    Private  102476     Bachelors  13       Never-married   \n",
       "\n",
       "                    6              7      8       9     10  11  12  \\\n",
       "9764     Craft-repair        Husband  White    Male      0   0  40   \n",
       "9765    Other-service  Not-in-family  White  Female      0   0  25   \n",
       "9766  Exec-managerial  Not-in-family  White    Male      0   0  30   \n",
       "9767    Other-service  Not-in-family  White  Female   3674   0  40   \n",
       "9768  Farming-fishing      Own-child  White    Male  27828   0  50   \n",
       "\n",
       "                 13  \n",
       "9764  United-States  \n",
       "9765  United-States  \n",
       "9766  United-States  \n",
       "9767  United-States  \n",
       "9768  United-States  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9764</th>\n",
       "      <td>30</td>\n",
       "      <td>Private</td>\n",
       "      <td>151868</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9765</th>\n",
       "      <td>32</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>104509</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9766</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>187592</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9767</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>49539</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>3674</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9768</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>102476</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>27828</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('auto_glucon': venv)"
  },
  "interpreter": {
   "hash": "8cbcbecefa271004e6582b9d6d4515a215f41598e734b38ac4197b8836b69e1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}