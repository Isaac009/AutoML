{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\" Example script for predicting columns of tables, demonstrating simple use-case \"\"\"\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# Training time:\n",
    "train_data = TabularDataset('../../modified/auto_imports-train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "train_data = train_data.head(500)  # subsample for faster demo\n",
    "print(train_data.head())\n",
    "label = 'class'  # specifies which column do we want to predict\n",
    "save_path = 'auto_imports_ms/'  # where to save trained models\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    0        1       2             3   4                   5  \\\n",
      "0  38  Private  181705     Assoc-voc  11  Married-civ-spouse   \n",
      "1  17  Private  121425          11th   7       Never-married   \n",
      "2  25  Private  460322     Bachelors  13       Never-married   \n",
      "3  25  Private  161007       HS-grad   9       Never-married   \n",
      "4  35  Private  204163  Some-college  10            Divorced   \n",
      "\n",
      "                   6          7      8       9  10  11  12             13  \\\n",
      "0    Exec-managerial    Husband  White    Male   0   0  40  United-States   \n",
      "1       Adm-clerical  Own-child  White  Female   0   0  16  United-States   \n",
      "2      Other-service  Own-child  White    Male   0   0  43  United-States   \n",
      "3  Machine-op-inspct  Own-child  White    Male   0   0  40  United-States   \n",
      "4  Machine-op-inspct  Unmarried  Black  Female   0   0  55  United-States   \n",
      "\n",
      "   class  \n",
      "0  <=50K  \n",
      "1  <=50K  \n",
      "2  <=50K  \n",
      "3  <=50K  \n",
      "4  <=50K  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=3600)\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"adult_ms/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  ['<=50K', '>50K']\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = >50K, class 0 = <=50K\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (>50K) vs negative (<=50K) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    520855.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.29 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['0', '2', '4', '10', '11', ...]\n",
      "\t\t('object', []) : 8 | ['1', '3', '5', '6', '7', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['1', '3', '5', '6', '7', ...]\n",
      "\t\t('int', [])      : 6 | ['0', '2', '4', '10', '11', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 400, Val Rows: 100\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.89s of the 3599.88s of remaining time.\n",
      "\t0.74\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.75s of the 3599.75s of remaining time.\n",
      "\t0.73\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.63s of the 3599.62s of remaining time.\n",
      "\t0.83\t = Validation accuracy score\n",
      "\t29.71s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3569.88s of the 3569.88s of remaining time.\n",
      "\t0.86\t = Validation accuracy score\n",
      "\t65.08s\t = Training runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3504.74s of the 3504.74s of remaining time.\n",
      "\t0.81\t = Validation accuracy score\n",
      "\t1.01s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3503.48s of the 3503.48s of remaining time.\n",
      "\t0.81\t = Validation accuracy score\n",
      "\t0.99s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3502.24s of the 3502.24s of remaining time.\n",
      "\t0.85\t = Validation accuracy score\n",
      "\t1.14s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3501.08s of the 3501.08s of remaining time.\n",
      "\t0.8\t = Validation accuracy score\n",
      "\t1.07s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3499.76s of the 3499.75s of remaining time.\n",
      "\t0.81\t = Validation accuracy score\n",
      "\t1.08s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3498.42s of the 3498.42s of remaining time.\n",
      "\t0.83\t = Validation accuracy score\n",
      "\t3.96s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3494.42s of the 3494.41s of remaining time.\n",
      "\t0.86\t = Validation accuracy score\n",
      "\t27.88s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ... Training model for up to 3466.49s of the 3466.49s of remaining time.\n",
      "\t0.77\t = Validation accuracy score\n",
      "\t4.65s\t = Training runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3461.69s of the 3461.69s of remaining time.\n",
      "\t0.89\t = Validation accuracy score\n",
      "\t267.58s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3192.24s of remaining time.\n",
      "\t0.89\t = Validation accuracy score\n",
      "\t0.46s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 408.24s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"adult_ms/\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         LightGBMLarge       0.89       0.026200  267.580235                0.026200         267.580235            1       True         13\n",
      "1   WeightedEnsemble_L2       0.89       0.026847  268.036947                0.000647           0.456712            2       True         14\n",
      "2               XGBoost       0.86       0.020559   27.883711                0.020559          27.883711            1       True         11\n",
      "3              LightGBM       0.86       0.044000   65.081026                0.044000          65.081026            1       True          4\n",
      "4              CatBoost       0.85       0.011787    1.142910                0.011787           1.142910            1       True          7\n",
      "5            LightGBMXT       0.83       0.030439   29.710629                0.030439          29.710629            1       True          3\n",
      "6       NeuralNetFastAI       0.83       0.031125    3.956893                0.031125           3.956893            1       True         10\n",
      "7      RandomForestEntr       0.81       0.225136    0.988075                0.225136           0.988075            1       True          6\n",
      "8      RandomForestGini       0.81       0.226806    1.011661                0.226806           1.011661            1       True          5\n",
      "9        ExtraTreesEntr       0.81       0.233344    1.077986                0.233344           1.077986            1       True          9\n",
      "10       ExtraTreesGini       0.80       0.226948    1.074177                0.226948           1.074177            1       True          8\n",
      "11       NeuralNetMXNet       0.77       0.144509    4.651050                0.144509           4.651050            1       True         12\n",
      "12       KNeighborsUnif       0.74       0.124773    0.004255                0.124773           0.004255            1       True          1\n",
      "13       KNeighborsDist       0.73       0.115733    0.004243                0.115733           0.004243            1       True          2\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'KNNModel', 'CatBoostModel', 'NNFastAiTabularModel', 'XTModel', 'TabularNeuralNetModel', 'WeightedEnsembleModel', 'LGBModel', 'XGBoostModel', 'RFModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) : 8 | ['1', '3', '5', '6', '7', ...]\n",
      "('int', [])      : 6 | ['0', '2', '4', '10', '11', ...]\n",
      "Plot summary of models saved to file: adult_ms/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Inference time:\n",
    "test_data = TabularDataset('../../modified/auto_imports-test.csv')  # another Pandas DataFrame\n",
    "y_test = test_data[label]\n",
    "test_data = test_data.drop(labels=[label], axis=1)  # delete labels from test data since we wouldn't have them in practice\n",
    "print(test_data.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loaded data from: ../../modified/adult-all-test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    0            1       2             3   4                   5  \\\n",
      "0  42      Private  350550  Some-college  10  Married-civ-spouse   \n",
      "1  24      Private  163053          11th   7       Never-married   \n",
      "2  49  Federal-gov   61885     Bachelors  13  Married-civ-spouse   \n",
      "3  21      Private  391312       HS-grad   9       Never-married   \n",
      "4  43      Private  133584  Some-college  10  Married-civ-spouse   \n",
      "\n",
      "                   6              7      8       9  10  11  12             13  \n",
      "0  Machine-op-inspct        Husband  White    Male   0   0  45  United-States  \n",
      "1              Sales  Not-in-family  White  Female   0   0  36  United-States  \n",
      "2    Exec-managerial        Husband  White    Male   0   0  45  United-States  \n",
      "3      Other-service  Not-in-family  Black  Female   0   0  30  United-States  \n",
      "4  Machine-op-inspct        Husband  White    Male   0   0  40  United-States  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "predictor = TabularPredictor.load(save_path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: accuracy on test data: 0.8177909714402702\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.8177909714402702,\n",
      "    \"balanced_accuracy\": 0.7089894016101054,\n",
      "    \"mcc\": 0.46550692313097847,\n",
      "    \"f1\": 0.5700483091787439,\n",
      "    \"precision\": 0.667420814479638,\n",
      "    \"recall\": 0.4974704890387858\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print('Accuracy = {:.2f}%'.format(perf[\"accuracy\"] * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 81.78%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "airline-passengers"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('auto_glucon': venv)"
  },
  "interpreter": {
   "hash": "8cbcbecefa271004e6582b9d6d4515a215f41598e734b38ac4197b8836b69e1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}