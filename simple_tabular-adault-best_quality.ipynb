{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\" Example script for predicting columns of tables, demonstrating simple use-case \"\"\"\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# Training time:\n",
    "train_data = TabularDataset('../../modified/adult-all-train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "train_data = train_data.head(500)  # subsample for faster demo\n",
    "print(train_data.head())\n",
    "label = 'class'  # specifies which column do we want to predict\n",
    "save_path = 'adult_bsq_ms/'  # where to save trained models\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    0        1       2             3   4                   5  \\\n",
      "0  38  Private  181705     Assoc-voc  11  Married-civ-spouse   \n",
      "1  17  Private  121425          11th   7       Never-married   \n",
      "2  25  Private  460322     Bachelors  13       Never-married   \n",
      "3  25  Private  161007       HS-grad   9       Never-married   \n",
      "4  35  Private  204163  Some-college  10            Divorced   \n",
      "\n",
      "                   6          7      8       9  10  11  12             13  \\\n",
      "0    Exec-managerial    Husband  White    Male   0   0  40  United-States   \n",
      "1       Adm-clerical  Own-child  White  Female   0   0  16  United-States   \n",
      "2      Other-service  Own-child  White    Male   0   0  43  United-States   \n",
      "3  Machine-op-inspct  Own-child  White    Male   0   0  40  United-States   \n",
      "4  Machine-op-inspct  Unmarried  Black  Female   0   0  55  United-States   \n",
      "\n",
      "   class  \n",
      "0  <=50K  \n",
      "1  <=50K  \n",
      "2  <=50K  \n",
      "3  <=50K  \n",
      "4  <=50K  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=3600, presets='best_quality')\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"adult_bsq_ms/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    500\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  ['<=50K', '>50K']\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = >50K, class 0 = <=50K\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (>50K) vs negative (<=50K) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    513639.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.29 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['0', '2', '4', '10', '11', ...]\n",
      "\t\t('object', []) : 8 | ['1', '3', '5', '6', '7', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 8 | ['1', '3', '5', '6', '7', ...]\n",
      "\t\t('int', [])      : 6 | ['0', '2', '4', '10', '11', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3599.89s of the 3599.89s of remaining time.\n",
      "\t0.742\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3599.76s of the 3599.75s of remaining time.\n",
      "\t0.696\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3599.62s of the 3599.62s of remaining time.\n",
      "\t0.856\t = Validation accuracy score\n",
      "\t210.35s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3389.1s of the 3389.1s of remaining time.\n",
      "\t0.854\t = Validation accuracy score\n",
      "\t299.81s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 3089.12s of the 3089.12s of remaining time.\n",
      "\t0.814\t = Validation accuracy score\n",
      "\t1.13s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3087.86s of the 3087.85s of remaining time.\n",
      "\t0.816\t = Validation accuracy score\n",
      "\t1.03s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 3086.69s of the 3086.68s of remaining time.\n",
      "\t0.848\t = Validation accuracy score\n",
      "\t7.16s\t = Training runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 3079.45s of the 3079.45s of remaining time.\n",
      "\t0.814\t = Validation accuracy score\n",
      "\t1.11s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 3078.19s of the 3078.19s of remaining time.\n",
      "\t0.808\t = Validation accuracy score\n",
      "\t1.08s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3076.97s of the 3076.97s of remaining time.\n",
      "No improvement since epoch 5: early stopping\n",
      "No improvement since epoch 9: early stopping\n",
      "\t0.842\t = Validation accuracy score\n",
      "\t6.55s\t = Training runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3070.21s of the 3070.21s of remaining time.\n",
      "\t0.814\t = Validation accuracy score\n",
      "\t120.74s\t = Training runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ... Training model for up to 2949.32s of the 2949.31s of remaining time.\n",
      "\t0.822\t = Validation accuracy score\n",
      "\t25.47s\t = Training runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2923.15s of the 2923.15s of remaining time.\n",
      "\t0.838\t = Validation accuracy score\n",
      "\t1353.73s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 1569.22s of remaining time.\n",
      "\t0.856\t = Validation accuracy score\n",
      "\t0.61s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2031.4s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"adult_bsq_ms/\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         LightGBMXT_BAG_L1      0.856       0.126040   210.348826                0.126040         210.348826            1       True          3\n",
      "1       WeightedEnsemble_L2      0.856       0.127336   210.955697                0.001296           0.606871            2       True         14\n",
      "2           LightGBM_BAG_L1      0.854       0.123396   299.809306                0.123396         299.809306            1       True          4\n",
      "3           CatBoost_BAG_L1      0.848       0.058365     7.159640                0.058365           7.159640            1       True          7\n",
      "4    NeuralNetFastAI_BAG_L1      0.842       0.139908     6.549142                0.139908           6.549142            1       True         10\n",
      "5      LightGBMLarge_BAG_L1      0.838       0.133240  1353.730155                0.133240        1353.730155            1       True         13\n",
      "6     NeuralNetMXNet_BAG_L1      0.822       0.661286    25.473271                0.661286          25.473271            1       True         12\n",
      "7   RandomForestEntr_BAG_L1      0.816       0.113361     1.033314                0.113361           1.033314            1       True          6\n",
      "8            XGBoost_BAG_L1      0.814       0.082661   120.743746                0.082661         120.743746            1       True         11\n",
      "9   RandomForestGini_BAG_L1      0.814       0.114760     1.128485                0.114760           1.128485            1       True          5\n",
      "10    ExtraTreesGini_BAG_L1      0.814       0.116661     1.114785                0.116661           1.114785            1       True          8\n",
      "11    ExtraTreesEntr_BAG_L1      0.808       0.114677     1.079096                0.114677           1.079096            1       True          9\n",
      "12    KNeighborsUnif_BAG_L1      0.742       0.114918     0.003451                0.114918           0.003451            1       True          1\n",
      "13    KNeighborsDist_BAG_L1      0.696       0.113034     0.004694                0.113034           0.004694            1       True          2\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_XGBoost', 'WeightedEnsembleModel', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNet', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_CatBoost'}\n",
      "Bagging used: True  (with 5 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) : 8 | ['1', '3', '5', '6', '7', ...]\n",
      "('int', [])      : 6 | ['0', '2', '4', '10', '11', ...]\n",
      "Plot summary of models saved to file: adult_bsq_ms/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Inference time:\n",
    "test_data = TabularDataset('../../modified/adult-all-test.csv')  # another Pandas DataFrame\n",
    "y_test = test_data[label]\n",
    "test_data = test_data.drop(labels=[label], axis=1)  # delete labels from test data since we wouldn't have them in practice\n",
    "print(test_data.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loaded data from: ../../modified/adult-all-test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    0            1       2             3   4                   5  \\\n",
      "0  42      Private  350550  Some-college  10  Married-civ-spouse   \n",
      "1  24      Private  163053          11th   7       Never-married   \n",
      "2  49  Federal-gov   61885     Bachelors  13  Married-civ-spouse   \n",
      "3  21      Private  391312       HS-grad   9       Never-married   \n",
      "4  43      Private  133584  Some-college  10  Married-civ-spouse   \n",
      "\n",
      "                   6              7      8       9  10  11  12             13  \n",
      "0  Machine-op-inspct        Husband  White    Male   0   0  45  United-States  \n",
      "1              Sales  Not-in-family  White  Female   0   0  36  United-States  \n",
      "2    Exec-managerial        Husband  White    Male   0   0  45  United-States  \n",
      "3      Other-service  Not-in-family  Black  Female   0   0  30  United-States  \n",
      "4  Machine-op-inspct        Husband  White    Male   0   0  40  United-States  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "predictor = TabularPredictor.load(save_path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: accuracy on test data: 0.8348858634455932\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.8348858634455932,\n",
      "    \"balanced_accuracy\": 0.7101107315052523,\n",
      "    \"mcc\": 0.5055163705770507,\n",
      "    \"f1\": 0.578961106760637,\n",
      "    \"precision\": 0.7601096641535298,\n",
      "    \"recall\": 0.4675379426644182\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print('Accuracy = {:.2f}%'.format(perf[\"accuracy\"] * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 83.49%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('auto_glucon': venv)"
  },
  "interpreter": {
   "hash": "8cbcbecefa271004e6582b9d6d4515a215f41598e734b38ac4197b8836b69e1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}