{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\" Example script for predicting columns of tables, demonstrating simple use-case \"\"\"\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "# Training time:\n",
    "train_data = TabularDataset('../../modified/abalone-train.csv')  # can be local CSV file as well, returns Pandas DataFrame\n",
    "train_data = train_data.head(500)  # subsample for faster demo\n",
    "print(train_data.head())\n",
    "label = 'class'  # specifies which column do we want to predict\n",
    "save_path = 'abalone_models/'  # where to save trained models\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loaded data from: ../../modified/abalone-train.csv | Columns = 9 / 9 | Rows = 3340 -> 3340\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   0      1      2      3       4       5       6       7  class\n",
      "0  M  0.650  0.525  0.185  1.4880  0.6650  0.3370  0.3780     11\n",
      "1  M  0.595  0.475  0.170  1.0965  0.4190  0.2290  0.3500     17\n",
      "2  M  0.575  0.470  0.140  0.8375  0.3485  0.1735  0.2400     11\n",
      "3  F  0.550  0.450  0.150  0.8750  0.3620  0.1755  0.2765     10\n",
      "4  M  0.505  0.385  0.145  0.6775  0.2360  0.1790  0.2000     15\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data, time_limit=3600)\n",
    "# NOTE: Default settings above are intended to ensure reasonable runtime at the cost of accuracy. To maximize predictive accuracy, do this instead:\n",
    "# predictor = TabularPredictor(label=label, eval_metric=YOUR_METRIC_NAME, path=save_path).fit(train_data, presets='best_quality')\n",
    "results = predictor.fit_summary()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"abalone_models/\"\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"abalone_models/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    500\n",
      "Train Data Columns: 8\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 20) unique label values:  [11, 17, 10, 15, 8, 13, 6, 5, 12, 9]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Updated label_count_threshold from 10 to 6 to avoid cutting too many classes.\n",
      "Warning: Some classes in the training set have fewer than 6 examples. AutoGluon will only keep 15 out of 20 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 6 examples that will be kept for training models: 0.982\n",
      "Train Data Class Count: 15\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    526079.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 7 | ['1', '2', '3', '4', '5', ...]\n",
      "\t\t('object', []) : 1 | ['0']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 1 | ['0']\n",
      "\t\t('float', [])    : 7 | ['1', '2', '3', '4', '5', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t8 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 392, Val Rows: 99\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.91s of the 3599.91s of remaining time.\n",
      "\t0.2222\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.79s of the 3599.78s of remaining time.\n",
      "\t0.2727\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3599.66s of the 3599.66s of remaining time.\n",
      "\t0.303\t = Validation accuracy score\n",
      "\t3.76s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3595.86s of the 3595.86s of remaining time.\n",
      "\t0.3434\t = Validation accuracy score\n",
      "\t1.24s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3594.54s of the 3594.54s of remaining time.\n",
      "\t0.303\t = Validation accuracy score\n",
      "\t1.41s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3593.11s of the 3593.11s of remaining time.\n",
      "\t0.2828\t = Validation accuracy score\n",
      "\t1.18s\t = Training runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3591.64s of the 3591.64s of remaining time.\n",
      "\t0.2828\t = Validation accuracy score\n",
      "\t1.22s\t = Training runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3590.14s of the 3590.14s of remaining time.\n",
      "\t0.303\t = Validation accuracy score\n",
      "\t1.56s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3588.57s of the 3588.57s of remaining time.\n",
      "\t0.2424\t = Validation accuracy score\n",
      "\t1.19s\t = Training runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3587.05s of the 3587.05s of remaining time.\n",
      "\t0.2323\t = Validation accuracy score\n",
      "\t1.19s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3585.53s of the 3585.53s of remaining time.\n",
      "\t0.303\t = Validation accuracy score\n",
      "\t179.99s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ... Training model for up to 3405.28s of the 3405.28s of remaining time.\n",
      "\t0.3131\t = Validation accuracy score\n",
      "\t3.98s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3401.16s of the 3401.16s of remaining time.\n",
      "\t0.3131\t = Validation accuracy score\n",
      "\t6.29s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3392.95s of remaining time.\n",
      "\t0.3535\t = Validation accuracy score\n",
      "\t0.41s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 207.47s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"abalone_models/\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2   0.353535       0.719679   13.348423                0.000476           0.407246            2       True         14\n",
      "1            LightGBMXT   0.343434       0.006627    1.235220                0.006627           1.235220            1       True          4\n",
      "2         LightGBMLarge   0.313131       0.009436    6.291453                0.009436           6.291453            1       True         13\n",
      "3        NeuralNetMXNet   0.313131       0.131122    3.977715                0.131122           3.977715            1       True         12\n",
      "4              CatBoost   0.303030       0.005111    1.562947                0.005111           1.562947            1       True          8\n",
      "5              LightGBM   0.303030       0.005962    1.411917                0.005962           1.411917            1       True          5\n",
      "6       NeuralNetFastAI   0.303030       0.019634    3.761804                0.019634           3.761804            1       True          3\n",
      "7               XGBoost   0.303030       0.020332  179.987680                0.020332         179.987680            1       True         11\n",
      "8      RandomForestGini   0.282828       0.219805    1.180615                0.219805           1.180615            1       True          6\n",
      "9      RandomForestEntr   0.282828       0.221261    1.217909                0.221261           1.217909            1       True          7\n",
      "10       KNeighborsDist   0.272727       0.115642    0.004968                0.115642           0.004968            1       True          2\n",
      "11       ExtraTreesGini   0.242424       0.222316    1.190114                0.222316           1.190114            1       True          9\n",
      "12       ExtraTreesEntr   0.232323       0.225346    1.188751                0.225346           1.188751            1       True         10\n",
      "13       KNeighborsUnif   0.222222       0.118186    0.003983                0.118186           0.003983            1       True          1\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'RFModel', 'CatBoostModel', 'XGBoostModel', 'XTModel', 'LGBModel', 'TabularNeuralNetModel', 'WeightedEnsembleModel', 'KNNModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) : 1 | ['0']\n",
      "('float', [])    : 7 | ['1', '2', '3', '4', '5', ...]\n",
      "Plot summary of models saved to file: abalone_models/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Inference time:\n",
    "test_data = TabularDataset('../../modified/abalone-test.csv')  # another Pandas DataFrame\n",
    "y_test = test_data[label]\n",
    "test_data = test_data.drop(labels=[label], axis=1)  # delete labels from test data since we wouldn't have them in practice\n",
    "print(test_data.head())\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loaded data from: ../../modified/abalone-test.csv | Columns = 9 / 9 | Rows = 836 -> 836\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   0      1      2      3       4       5       6      7\n",
      "0  F  0.440  0.355  0.115  0.4150  0.1585  0.0925  0.131\n",
      "1  M  0.680  0.515  0.170  1.6115  0.8415  0.3060  0.395\n",
      "2  F  0.475  0.360  0.125  0.4470  0.1695  0.0810  0.140\n",
      "3  M  0.620  0.480  0.160  1.0765  0.4120  0.2530  0.300\n",
      "4  M  0.515  0.435  0.145  0.8815  0.2920  0.2060  0.255\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "predictor = TabularPredictor.load(save_path)  # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluation: accuracy on test data: 0.22966507177033493\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.22966507177033493,\n",
      "    \"balanced_accuracy\": 0.10508099528629468,\n",
      "    \"mcc\": 0.12106042409142431\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print('Accuracy = {:.2f}%'.format(perf[\"accuracy\"] * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 22.97%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('auto_glucon': venv)"
  },
  "interpreter": {
   "hash": "8cbcbecefa271004e6582b9d6d4515a215f41598e734b38ac4197b8836b69e1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}